<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>身为低保户还要写博客是不是搞错了什么</title>
    <url>/2021/06/29/%E8%BA%AB%E4%B8%BA%E4%BD%8E%E4%BF%9D%E6%88%B7%E8%BF%98%E8%A6%81%E5%86%99%E5%8D%9A%E5%AE%A2%E6%98%AF%E4%B8%8D%E6%98%AF%E6%90%9E%E9%94%99%E4%BA%86%E4%BB%80%E4%B9%88/</url>
    <content><![CDATA[<p>没有枪（指域名），没有炮（指服务器），只有一杆冲锋号（穷学生一个）。该如何搭建一个属于自己的博客？</p>
<p>个人建站明显不可能了，而又想定制外观、乱折腾的话，其实也就只剩 博客园 和 Hexo 两个选择了。</p>
<p>最终我选择了在 GitHub Pages 上搭建基于 Hexo 的静态网页博客，主要原因还是希望能多折腾，否则博客园无疑是个更好的选择。接下来本文就会详细介绍下如何使用 Hexo + Github 搭建博客的过程。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/image-20210705115752219.png" alt="image-20210705115752219"></p>
<h3 id="什么是-Hexo"><a href="#什么是-Hexo" class="headerlink" title="什么是 Hexo?"></a>什么是 <a href="https://hexo.io/zh-cn/">Hexo</a>?</h3><blockquote>
<p>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 <a href="http://daringfireball.net/projects/markdown/">Markdown</a>（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p>
</blockquote>
<h3 id="什么是静态网页"><a href="#什么是静态网页" class="headerlink" title="什么是静态网页"></a>什么是静态网页</h3><p>本人的理解是：静态网页并非静止不动的网页，而是指除了通过 URL 获得网页信息就没有其他方法<strong>与服务器</strong>交互的网页。</p>
<h3 id="什么是-GitHub-Pages？"><a href="#什么是-GitHub-Pages？" class="headerlink" title="什么是 GitHub Pages？"></a>什么是 <a href="https://pages.github.com/">GitHub Pages</a>？</h3><blockquote>
<p>GitHub Pages 是由 GitHub 官方提供的一种免费的静态站点托管服务，让我们可以在 GitHub 仓库里托管和发布自己的静态网站页面用于介绍和宣传自己的项目。</p>
</blockquote>
<h3 id="什么是-GitHub-Pages-Hexo"><a href="#什么是-GitHub-Pages-Hexo" class="headerlink" title="什么是 GitHub Pages + Hexo?"></a>什么是 GitHub Pages + Hexo?</h3><p>即将 Hexo 生成的静态网页部署在 GitHub Pages。我不太清楚 GitHub 官方对用户蹭服务器写博客的态度如何，但从大量采用这个方式的个人博客和 <a href="https://hexo.io/zh-cn/">Hexo</a> 官网的介绍来看，这一行为最起码是被长期默许的。整个过程如下图</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/Hexo-GitHubPages流程.png" alt="Hexo-GitHubPages流程"></p>
<ul>
<li>优点：完全免费；可自由选择主题外观；静态网页；维护稳定</li>
<li>缺点：相比动态博客，发文不够便利；Github 在国内访问日常性抽风</li>
</ul>
<h2 id="如何使用-GitHub-Pages-Hexo"><a href="#如何使用-GitHub-Pages-Hexo" class="headerlink" title="如何使用 GitHub Pages + Hexo?"></a>如何使用 GitHub Pages + Hexo?</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><ul>
<li><a href="https://link.zhihu.com/?target=https%3A//nodejs.org/zh-cn">Node.js</a> ：Hexo 基于 Node.js，并使用 npm 安装 （Node.js 自带）</li>
<li><a href="https://link.zhihu.com/?target=https%3A//git-scm.com/downloads">Git</a> ：部署到 GitHub Pages 上，同时 Hexo 自身也需要 Git 来下载</li>
</ul>
<h3 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h3><ol>
<li>没有账号先注册 GitHub 账号，CMD 输入以下指令，用于设置用户名和邮箱（GitHub 账号所使用邮箱）：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;GitHub 用户名&quot;</span><br><span class="line">git config --global user.email &quot;GitHub 邮箱&quot;</span><br></pre></td></tr></table></figure>
<ol>
<li>创建并添加 SSH 密钥（为了 GitHub 服务器验证身份，从而使得 Hexo 可以自动 Push），CMD 输入以下指令 :</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;GitHub 邮箱&quot;</span><br></pre></td></tr></table></figure>
<p>一路回车，最后在 C:\Users\用户名\.ssh 目录（要勾选显示“隐藏的项目”），用记事本打开 id_rsa.pub，并复制内容，在 GitHub 网页上的个人设置中添加 SSH keys，将之前复制的内容添加进去。</p>
<ol>
<li>最后在 CMD 内输入：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>
<p>显示 “Hi xxx! You’ve successfully……” 则成功。</p>
<ol>
<li>创建一个 GitHub 仓库（public）用于发布博客。</li>
</ol>
<h3 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h3><p>创建一个文件夹，该文件夹用于你日后本地原文件的存储，然后 CMD 内 cd 到该文件夹下，并输入以下指令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli  # 安装 Hexo</span><br><span class="line">hexo init                # 初始化</span><br><span class="line">npm install              # 安装组件</span><br><span class="line">hexo g                   # 生成页面</span><br><span class="line">hexo s                   # 启动预览</span><br></pre></td></tr></table></figure>
<p>然后访问 <code>http://localhost:4000</code> ，出现 Hexo 默认初始化界面。</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/image-20210707005908359.png" alt="image-20210707005908359"></p>
<h3 id="Hexo-GitHub-Pages"><a href="#Hexo-GitHub-Pages" class="headerlink" title="Hexo + GitHub Pages"></a>Hexo + GitHub Pages</h3><p>首先安装 hexo-deployer-git，Hexo 用于部署到类GitHub网站上的插件（Gitee 也可以，似乎能解决国内访问困难的问题）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p>然后修改全局 _config.yml 的 deploy 项如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:用户名/仓库名</span><br><span class="line">  branch: main # 网上有些老教程写的 master，但是Github早就因为政治正确把默认分支的名字改名了</span><br></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>以后每次要发布新文章或者修改就，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new &quot;&lt;name&gt;&quot;     # 新建文章</span><br></pre></td></tr></table></figure>
<p>然后 source 文件夹中会出现一个 \<name>.md 文件，就可以使用 Markdown 编辑器在该文件中撰写文章了，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo g                # 生成页面</span><br><span class="line">hexo s                # 本地预览</span><br></pre></td></tr></table></figure>
<p>在 <code>http://localhost:4000</code> 打开预览，没有问题就可以部署了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo d                # 部署</span><br></pre></td></tr></table></figure>
<h2 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h2><p>本博客使用的主题为 <a href="https://github.com/Fechin/hexo-theme-diaspora">hexo-theme-diaspora</a> ，具体方法参考该链接。</p>
<p>这个主题也可以说是比较小众又年久失修，俘获我的主要原因就是主页的第一眼，然而实际用起来真是哪哪哪都有问题。如果需要功能完善的主题，可以选择 <a href="http://theme-next.iissnan.com/">NexT</a> 这样用的人多的主题。</p>
<p>在这里也大胆地写一下关于这个主题的 todo-list：</p>
<ol>
<li>统计网页访问量等（当众处刑；</li>
<li>音乐音量调节功能；</li>
<li>漂浮且不会把文章挤到边上的 TOC ；</li>
<li>支持更多的 Markdown 第一方及第三方语法（目前连<strong>粗体</strong>都不支持）。</li>
</ol>
]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>diaspora</tag>
      </tags>
  </entry>
  <entry>
    <title>用于改进任意风格迁移的归一化损失</title>
    <url>/2021/07/07/%E7%94%A8%E4%BA%8E%E6%94%B9%E8%BF%9B%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96%E6%8D%9F%E5%A4%B1/</url>
    <content><![CDATA[<p>本文为 <a href="https://arxiv.org/abs/2104.10064">Style-Aware Normalized Loss for Improving Arbitrary Style Transfer</a> 的概览导读</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="现有的任意风格迁移-Arbitrary-Style-Transfer—AST-方法："><a href="#现有的任意风格迁移-Arbitrary-Style-Transfer—AST-方法：" class="headerlink" title="现有的任意风格迁移(Arbitrary Style Transfer—AST)方法："></a>现有的任意风格迁移(Arbitrary Style Transfer—AST)方法：</h3><ol>
<li><a href="https://research.google/teams/brain/magenta/">Magenta – Google Research</a> ❓</li>
<li><a href="https://arxiv.org/pdf/1703.06868.pdf">AdaIN</a> 基于模型迭代的方法</li>
<li><a href="https://arxiv.org/abs/1808.04537">LinearTransfer</a> 通过前馈神经网络学习获得一个风格转移矩阵 learns the transformation matrix with a feed-forward network</li>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Arbitrary_Style_Transfer_With_Style-Attentional_Networks_CVPR_2019_paper.pdf">SANet</a> 基于模型迭代的方法</li>
</ol>
<h3 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h3><p>① 欠风格化（Under-Stylization）+ ② 过风格化（Over-Stylization）= 风格化不平衡 （imbalanced style transferability —— IST）</p>
<h3 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h3><p>训练样例中不同风格迁移难度不一</p>
<h3 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h3><p>风格认知归一化损失</p>
<h2 id="发掘问题"><a href="#发掘问题" class="headerlink" title="发掘问题"></a>发掘问题</h2><p>本文改进的损失函数是传统的 NST (Neural Style Transfer) Loss</p>
<script type="math/tex; mode=display">
\mathcal{L} = \mathcal{L}_{c}(C,P) + \beta\mathcal{L}_{s}(S,P)\\</script><p>分为内容损失函数和风格损失函数两个部分</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{L}_{c}(C,P) &=  MSE(\mathcal{F}(C),\mathcal{F}(P))\\
\mathcal{L}_{s}(C,P) &=  MSE(\mathcal{G}\circ\mathcal{F}(C),\mathcal{G}\circ\mathcal{F}(P))
\end{align*}</script><p>其中 $C$ 为内容图，$S$ 为风格图，$P$ 为生成图，$\mathcal{F}$ 为 VGG 网络的特征输出，$\mathcal{G}$ 为格拉姆矩阵；即基于图像迭代的风格迁移方法最基本的损失函数，本文使用其来训练其他模型</p>
<h3 id="实验一-mathcal-L-s-分布"><a href="#实验一-mathcal-L-s-分布" class="headerlink" title="实验一 $\mathcal{L}_{s}$ 分布"></a>实验一 $\mathcal{L}_{s}$ 分布</h3><p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021-06/风格损失分布.png" alt="风格损失分布"></p>
<p>上图为针对四种模型，风格损失的分布，得到以下两点：</p>
<ol>
<li>不同模型 $\mathcal{L}_{s}$ 分布相似；</li>
<li>与直觉相反，$\mathcal{L}_{s}$ 高反而过风格化，不能反映风格化的程度。</li>
</ol>
<h3 id="实验二-mathcal-L-s-VS-人工评价"><a href="#实验二-mathcal-L-s-VS-人工评价" class="headerlink" title="实验二 $\mathcal{L}_{s}$ VS 人工评价"></a>实验二 $\mathcal{L}_{s}$ VS 人工评价</h3><p>$\mathcal{L}_{s}$ 越大，人工评价反而越好</p>
<p>如果将人工评价定义为三个等级 GOOD(-1)，OK(0)，BAD(1)，则如下表数据，人工评价和 $\mathcal{L}_{s}$ 是负相关</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021-06/风格损失vs人工评价.png" alt="风格损失vs人工评价"></p>
<h2 id="探讨原因"><a href="#探讨原因" class="headerlink" title="探讨原因"></a>探讨原因</h2><p>在以 Batch 为单位的训练中，如下式</p>
<script type="math/tex; mode=display">
\mathcal{L}_{s}^{Batch} = \sum_{k\in{1,...,B}} \frac{1}{B}\cdot \mathcal{L}_{s}(S_k,P_k)</script><p>由于不同风格的迁移难度不同（在格拉姆矩阵的评价指标下），换言之实际风格迁移效果与 $\mathcal{L}_s$ 不匹配，不同风格的 $\mathcal{L}_s$ 相差上千倍，经过平均后，大的 $\mathcal{L}_s$ 比重太大，导致只针对这些风格迭代模型，使得 $\mathcal{L}_s$ 高的风格过风格化， $\mathcal{L}_s$ 低的风格欠风格化</p>
<h2 id="改进方法"><a href="#改进方法" class="headerlink" title="改进方法"></a>改进方法</h2><p>原文作者使用了一个简单的归一化方法，将 $\mathcal{L}_{s}$ 除以其上界，如下式</p>
<script type="math/tex; mode=display">
\hat{\mathcal{L}}_{s}=\frac{\mathcal{L}_{s}}{sup\{\mathcal{L}_{s}\}}</script><p>其中 $sup{\mathcal{L}_{s}}$ 为 $\mathcal{L}_s$ 的上届，如下式</p>
<script type="math/tex; mode=display">
sup\{\mathcal{L}_{s}(S,P)\} =\frac{||\mathcal{G}\circ\mathcal{F}(S)||^2+||\mathcal{G}\circ\mathcal{F}(P)||^2}{N}</script><p>其依据是 $\mathcal{L}_{s}$ 和其上界的高度相关性</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/Ls和其上界的相关性.png" alt="Ls和其上界的相关性" style="zoom:67%;" /></p>
<h2 id="效果预期"><a href="#效果预期" class="headerlink" title="效果预期"></a>效果预期</h2><p>一个简单的改进，直接逆转了原本损失函数的缺陷</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021-06/改进后-风格损失分布.png" alt="改进后-风格损失分布"></p>
<p>该分布图为原本模型使用归一化风格损失输出的结果，可见大的 $\hat{\mathcal{L}}_s$ 对应欠风格化的图片，小的 $\hat{\mathcal{L}}_s$ 对应过风格化的图片，可以预见使用该函数训练模型将纠正风格化不平衡的问题，具体效果可见原论文，可以谓之立竿见影<img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021-06/改进后-风格损失vs人工评价.png" alt="改进后-风格损失vs人工评价"></p>
<p>人工评价也和 $\hat{\mathcal{L}}_{s}$ 是正相关的关系</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文的创新起到了立竿见影的效果，然而究其创新可以说是普普通通，问题很明显，方法也很简单，绝对不存在什么奇思妙想或者开创之举，但效果确实如此明显，为何如今才有人提出。暂能想到的两个原因：</p>
<ol>
<li>通过格拉姆矩阵来计算风格相似度的方法，略老掉牙了，如今的研究热点早就跑到了各种对抗神经网络、判别器、生成器上了；</li>
<li>计算机视觉发展日新月异，很多细节都很粗糙，有大量明显的可优化点都被忽视了。</li>
</ol>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/image-20210707211703072.png" alt="结果"></p>
]]></content>
      <categories>
        <category>paper overview</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Git</title>
    <url>/2021/07/14/Hello-Git/</url>
    <content><![CDATA[<p>本系列文章为本人学习 Git 的笔记，以供个人纪录和分享。</p>
<p>使用的教材为 <a href="http://iissnan.com/progit/">Pro Git</a> 。</p>
<h2 id="什么是版本控制？"><a href="#什么是版本控制？" class="headerlink" title="什么是版本控制？"></a>什么是版本控制？</h2><blockquote>
<p>版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。</p>
</blockquote>
<p>如果有以下几种情况那么你就可能会需要它：</p>
<ol>
<li>你工作相关的文件不止一个，且互相关联。假想以下情况，你修改了文件A，同时由于相关性你也修改了文件B，当你后悔这个修改的时候，你就要将这两个文件都返回为未修改的状态，然而如何确保你不忘记修改文件B？又如何确保你能将两个文件退回到一个相同的版本？这时候你就会需要版本控制；</li>
<li>你的甲方有众多需求且易变，使得你不得不开发众多版本，为了协调好这些版本中不同和公用的部分，你也需要版本控制；</li>
<li>你工作需要团队协作，为了更好地同步工作进度，并且避免团队中个别成员的失误造成不可逆的影响，又想让每个成员都可以以完整的项目继续功能测试，那么你就需要版本控制；</li>
</ol>
<p>总而言之，版本控制系统的初衷是为了解决项目开发中的复杂性，其中包括项目自身和人员等方面；这也意味着无脑使用版本控制也并非好事，其本身也会增加复杂性。版本控制系统可以分为三大类：本地版本控制系统；集中化的版本控制系统；分布式版本控制系统。</p>
<h3 id="本地版本控制系统"><a href="#本地版本控制系统" class="headerlink" title="本地版本控制系统"></a>本地版本控制系统</h3><p>很多人工作的文件夹中有类似“初版”、“一改”、“二改”、“改中改之最终改”等目录名，用以记录和区分各个版本。后来为了避免手动备份中不可避免的各种失误，开发了本地版本控制系统。</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/image-20210713001144270.png" alt="本地版本控制系统"></p>
<p>其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。值得注意的是该版本控制系统靠着记录每次文件的修改来保存不同版本的文件。</p>
<h3 id="集中化的版本控制系统"><a href="#集中化的版本控制系统" class="headerlink" title="集中化的版本控制系统"></a>集中化的版本控制系统</h3><blockquote>
<p> 为了让不同系统上的开发者协同工作，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。</p>
</blockquote>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/image-20210713002312936.png" alt="集中化的版本控制系统"></p>
<p>而数据集中也会带来风险集中的问题，主机宕机和数据丢失都会造成巨大问题，依靠员工碰巧保存的数据（图上的 Checkout ）由于没有版本记录，既无法确认其具体版本，也无法用于重建完整的版本控制系统。</p>
<h3 id="分布式版本控制系统"><a href="#分布式版本控制系统" class="headerlink" title="分布式版本控制系统"></a>分布式版本控制系统</h3><blockquote>
<p>于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份</p>
</blockquote>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/image-20210713004052781.png" alt="分布式版本控制系统"></p>
<p>在原书的语境中将这三种系统的关系视作一个不断上升的过程，本人认为不尽然。相对本地版本控制系统，后两者确实是进步；然而集中式和分布式则不是如此，分布式固然更加灵活更加可靠，但权限上只能限制开发者的提交和修改，从原理上并无法限制开发者获取项目文件，这对开源软件或许没有问题，然而对于闭源软件（没有额外的安全措施下），则增加了数据泄露的风险。并不能通过目前软件开发中 Git 使用最广泛，就认为其是万能的银弹。</p>
<h2 id="什么是-Git-？"><a href="#什么是-Git-？" class="headerlink" title="什么是 Git ？"></a>什么是 <a href="https://git-scm.com/">Git</a> ？</h2><p>Git 是一个开源的分布式版本控制系统。其诞生于 Linux 开源社区的需求中，Linux 的缔造者 Linus Torvalds 对其制定了以下目标：</p>
<ol>
<li>速度</li>
<li>简单的设计</li>
<li>对非线性开发模式的强力支持</li>
<li>完全分布式</li>
<li>有能力高效管理类似 Linux 内核一样的超大规模项目</li>
</ol>
<h3 id="时刻保持数据完整性"><a href="#时刻保持数据完整性" class="headerlink" title="时刻保持数据完整性"></a>时刻保持数据完整性</h3><p>Git 使用 SHA-1 算法计算数据的校验和作为文件的指纹，不同版本的不同文件都保有其唯一的指纹。Git 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名，其由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">24b9da6552252987aa493b52f8696cd6d3b00373</span><br></pre></td></tr></table></figure>
<p>文件索引由文件内容决定，这样子 Git 便可以轻易识别文件内容是否发生改变，并轻易确保文件的一致性。</p>
<h3 id="直接记录文件副本"><a href="#直接记录文件副本" class="headerlink" title="直接记录文件副本"></a>直接记录文件副本</h3><p>Linux 直接存储文件的快照，而不是像其他版本控制系统一样关注文件的改动，这为 Git 带来了速度和稳健性，但当一个大文件有一个小修改的时候也会整体备份一边，导致其可能有些滥用存储空间，然而如今的电脑在除极少数情况下，时间复杂度远比空间复杂度来得重要（硬盘总比 CPU 便宜）。开发者每次提交更新时候，Git 会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/18333fig0105-tn.png" alt="Git 保存每次更新时的文件快照"></p>
<blockquote>
<p>稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。</p>
</blockquote>
<h3 id="三个区域、三个状态"><a href="#三个区域、三个状态" class="headerlink" title="三个区域、三个状态"></a>三个区域、三个状态</h3><p>在 Git 中任何文件都只有三种状态：已提交（committed）、已修改（modified）以及已暂存（staged）。同时 Git 有三个工作区域：Git 的工作目录（working directory），暂存区域（staging area）以及本地仓库（repository）。其关系如下：</p>
<ol>
<li>在工作目录中修改某些文件，其状态变为已修改；</li>
<li>对修改后的文件进行快照，保存到暂存区域，其状态变为已暂存；</li>
<li>提交更新，将暂存区的文件快照永久转储到 Git 的本地仓库中，其状态变为以提交。</li>
</ol>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/07/18333fig0106-tn.png" alt="img"></p>
]]></content>
      <categories>
        <category>Hello Git</category>
      </categories>
      <tags>
        <tag>Tools</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵论1：概述和LU分解</title>
    <url>/2021/09/23/%E7%9F%A9%E9%98%B5%E8%AE%BA1%EF%BC%9A%E6%A6%82%E8%BF%B0%E5%92%8CLU%E5%88%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="矩阵论介绍"><a href="#矩阵论介绍" class="headerlink" title="矩阵论介绍"></a>矩阵论介绍</h2><p>上位数学系课程：泛函，群</p>
<p>前置课程：高数，线性代数</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/09/image-20210923121330773.png" alt="image-20210923121330773"></p>
<p>数是特殊的矩阵 =&gt; 矩阵没有部分数的属性（例：乘法交换律）</p>
<p>矩阵的三个层面：</p>
<ol>
<li>微观层面：数的组合</li>
<li>宏观层面：完全抽象物，符合某些运算的数学对象（上位数学系视角）</li>
<li>中间层面：向量组</li>
</ol>
<hr>
<h2 id="LU分解"><a href="#LU分解" class="headerlink" title="LU分解"></a>LU分解</h2><p>为了引出 LU 分解的意义和解法，让我们先回顾一下线性方程的基本解法。</p>
<h3 id="线性方程组解法（线性代数范围）"><a href="#线性方程组解法（线性代数范围）" class="headerlink" title="线性方程组解法（线性代数范围）"></a>线性方程组解法（线性代数范围）</h3><ol>
<li>Gauss 消元法</li>
<li>求逆矩阵<ol>
<li>Gauss 消元法</li>
<li>伴随矩阵</li>
</ol>
</li>
<li>克莱姆法则</li>
</ol>
<p>其中克莱姆法则和伴随矩阵复杂度为 O(n*n!)；Gauss消元法和求逆矩阵法则为 O(n^3)。</p>
<p>为了之后求解和证明 LU 分解在此先介绍一个定义</p>
<p><strong>主元 pivot ：在矩阵消去（例如Gauss消元法）过程中，每列的要保留的非零元素，用它可以把该列其他消去。</strong></p>
<hr>
<h3 id="L-D-U-分解定理："><a href="#L-D-U-分解定理：" class="headerlink" title="L(D)U 分解定理："></a>L(D)U 分解定理：</h3><p>如果方阵 A 的各阶<strong>顺序主子式</strong>，</p>
<script type="math/tex; mode=display">
\Delta_k\neq 0\ \ k=1,2...n</script><p>则存在唯一的主对角线上元素全为1的下三角矩阵（称为单位下三角元素）L 与唯一的<strong>非奇异</strong>上三角矩阵 U，使得</p>
<script type="math/tex; mode=display">
A = LU</script><p>或者存在唯一的单位下三角元素 L、单位上三角元素 D 和 对角矩阵，使得</p>
<script type="math/tex; mode=display">
A=LDU</script><p>注意，在矩阵乘法结合律和 LU 分解定理的基础上，LDU 分解定理是显然的。</p>
<hr>
<h3 id="LU-分解求法："><a href="#LU-分解求法：" class="headerlink" title="LU 分解求法："></a>LU 分解求法：</h3><p>由于 LU 分解定理的证明要涉及到求法，所以先介绍 LU 分解求法。</p>
<script type="math/tex; mode=display">
\begin{align}
&A=LU\  and\ L^{-1}L=I \\ \\
\Rightarrow & L^{-1}A = L^{-1}LU=U \\ \\
\Rightarrow & L^{-1}(A,I) = (L^{-1}A,L^{-1})=(U,L^{-1})
\end{align}</script><p>上面的证明过程表明，通过一系列行初等变换，即 $L^{-1}$ 分解为的初等矩阵，可以将增广矩阵 $(A,I)$ 转化为 $(U,L^{-1})$ ，接下来在<strong>求 $L^{-1}$ 的逆即可得到  $L$</strong> </p>
<p>⭐ 这里介绍一种简单的  $L$ 求法，每次矩阵消去之前都将主元以及其下方元素记下，最后各列除以其主元，即为 $L$，证明略</p>
<p>⭐ 思考：这里能否使用所有的三类初等变换？</p>
<p>答案：只能使用第三类初等变换，由于 $L^{-1}$ 为单位下三角矩阵，其只由第三类初等变换矩阵构成</p>
<hr>
<h3 id="LU-分解定理证明"><a href="#LU-分解定理证明" class="headerlink" title="LU 分解定理证明"></a>LU 分解定理证明</h3><p>1.必要性 $A=LU\Rightarrow \Delta_k\neq0$</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{pmatrix} 
=
\begin{pmatrix}
L_{11} & 0 \\
L_{21} & L_{22}
\end{pmatrix} 
\begin{pmatrix}
U_{11} & U_{12} \\
0 & U_{22}
\end{pmatrix} 
=
\begin{pmatrix}
L_{11}U_{11} & L_{11}U_{12} \\
L_{21}U_{11} & L_{21}U_{12}+L_{22}U_{22}
\end{pmatrix}</script><script type="math/tex; mode=display">
\begin{align}
\Rightarrow det(A_{11}) &= det(L_{11}U_{11})\\ \\
&=det(L_{11})det(U_{11})\\ \\
&=1\cdot det(U_{11}) \neq 0
\end{align}</script><p>考虑到 $A_{11}$ 阶数的任意性，以上证明显然对 $A$ 的任意顺序主子式都有效。</p>
<p>2.充分性 $A=LU\Leftarrow \Delta_k\neq0$</p>
<p>证明充分性可以从 LU分解求法 入手，可以观察到阻碍任意方阵 LU分解 的唯一障碍，在于主元为 0。</p>
<p>而 LU分解求法 中，主元左侧的元素全为 0，而 $\Delta_k\neq0$ 和初等变换不会改变行列式的非零性，可知主元必不为 0。</p>
<p>所以符合 $\Delta_k\neq0$ 条件的矩阵一定可以通过上述 LU分解求法 得到 $A=LU$。</p>
<hr>
<h3 id="LU-分解用途"><a href="#LU-分解用途" class="headerlink" title="LU 分解用途"></a>LU 分解用途</h3><p>当 $A=LU$ 时，方程 $Ax=b$ 可以写成 $L(Ux)=b$ ，令 $Ux=y$ ,则有：</p>
<script type="math/tex; mode=display">
Ly=b \ \ Ux=y</script><p>即将原本单步问题化为两步，因为矩阵 $L$ 和 $U$ 都是三角矩阵，所以求解上述两个方程比直接求解 $A=LU$ 简单，其复杂度为 O(n^2) 。</p>
<p>然而LU分解的复杂度为 O(n^3)，似乎LU分解并不能减少运算量，但LU分解最大的优点在于复用性，其第一步LU分解，仅与 $A$ 有关，和 $b$ 无关，这意味着不论 $b$ 如何改变，重复做多少次，其增加的复杂度都是 O(n^2)。</p>
<p>当然LU分解法也不是只有好处的，一个明显的缺点为LU只适用于解所有顺序主子式都大于0的，通用性欠缺，但这也可以通过某些方法规避，将在下文介绍。</p>
<hr>
<h2 id="LU分解的几种推广"><a href="#LU分解的几种推广" class="headerlink" title="LU分解的几种推广"></a>LU分解的几种推广</h2><h3 id="带置换的LU分解——PLU分解"><a href="#带置换的LU分解——PLU分解" class="headerlink" title="带置换的LU分解——PLU分解"></a>带置换的LU分解——PLU分解</h3><p>如果存在置换矩阵 P、单位下三角矩阵 L 与 上三角矩阵 U，使得方阵 A 满足</p>
<script type="math/tex; mode=display">
PA = LU</script><p>则称上式为 A 的带置换 P 的 LU分解</p>
<p>具体 P 的选择方法可以使用列选主元法：在每列确定主元的时候，都选择该列主元位置及其下方<strong>绝对值最大</strong>的元素。</p>
<p>这一方法有两个作用：</p>
<p>1.可以解决上文所说的缺陷—— LU 分解只适用于解所有顺序主子式都大于0的；</p>
<p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/09/列选主元法保证主元不为0.png" alt="列选主元法保证主元不为0" style="zoom:67%;" /></p>
<p>上图为 LU 分解求法中的某个步骤，蓝色为已经选择的主元，白色为0，绿色为供选择下一个的主元的区域。</p>
<p>简单证明：若绿色全为0，则红色区域由于不为方阵，一定可以通过初等变换获得一个全为0的行，左侧又全为0，即整个矩阵通过初等变换得到一个全为0的行，与矩阵可逆矛盾，所以绿色区域一定存在一个不为0的元素，通过列选主元法得到的主元一定不为0。</p>
<p>2.由于保证消去时候的乘子都不超过1，抑制了数据误差的放大，提高计算稳定性。</p>
<h4 id="非可逆方阵"><a href="#非可逆方阵" class="headerlink" title="非可逆方阵"></a>非可逆方阵</h4><p>分为两种情况：</p>
<ol>
<li>不满秩（秩为 r ）方阵：方法和可逆方阵完全一致，只是 U 方阵对角线会有 n-r 个 0；</li>
<li>长方形（m, n）矩阵：方法和可逆方阵相似，只是 L、U 不为方阵，L 为（m, min(m,n)），U 为（min(m,n), n）。</li>
</ol>
<hr>
<h3 id="Python-库中的-LU-分解"><a href="#Python-库中的-LU-分解" class="headerlink" title="Python 库中的 LU 分解"></a>Python 库中的 LU 分解</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 调用库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.linalg <span class="keyword">import</span> lu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义矩阵</span></span><br><span class="line">A = np.array([[<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">6</span>,<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认使用 PLU 分解，并允许本文中的几种推广 LU 分解</span></span><br><span class="line">P,L,U = lu(A)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="考点"><a href="#考点" class="headerlink" title="考点"></a>考点</h3><ol>
<li>LU 分解求法</li>
</ol>
]]></content>
      <categories>
        <category>MatrixTheory</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵论2：线性空间</title>
    <url>/2021/10/06/%E7%9F%A9%E9%98%B5%E8%AE%BA2%EF%BC%9A%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/</url>
    <content><![CDATA[<p>数形结合对于<strong>非数学系</strong>学生理解和<strong>应用</strong>矩阵论知识至关重要</p>
<h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><h4 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h4><p>笛卡尔积：</p>
<script type="math/tex; mode=display">
\def\bm{\boldsymbol}
A \times B \overset{def}{=} \{ (a,b)|a\in A,b \in B \}</script><p>映射：对于 $S$ 和 $S’$ 两个集合，如果存在一个法则 $f$ ，使得 $S$ 中的每一个元素 $s$ 都有 $S’$ 中唯一确定的一个元素$s’$ 与它对应，则称 $f$ 是 $S$ 到 $S’$ 的一个映射，记作 </p>
<script type="math/tex; mode=display">
\begin{align}
f: S &\rightarrow S'
\\
\alpha &\mapsto \beta
\end{align}</script><p>代数运算：对于非空集合 $A,B,C$ ，定义一个如下映射，</p>
<script type="math/tex; mode=display">
\begin{align}
f: A\times B &\rightarrow C
\\
(a,b) &\mapsto c
\end{align}</script><p>这种映射被称为 $A\times B$ 到 $C$ 的<strong>代数运算</strong>。当 $A,B,C$ 均为 $S$ 时，这种映射也被称为 $S$ 上的（二元）代数运算，例如实数加法至于实数域，称这种运算对于 $S$ 封闭。</p>
<hr>
<h4 id="线性空间-1"><a href="#线性空间-1" class="headerlink" title="线性空间"></a>线性空间</h4><p>定义：设 $V$ 是一个非空集合，$F$ 是一个数域（常见的有实数域 $\mathbb R$ 和复数域 $\mathbb C$）。在 $V$ 上定义了一种代数运算：$(v,v)\mapsto v$ ，叫做加法；再定义一种 $F\times V$ 到 $V$ 的代数运算：$(k,v)\mapsto v$ ，叫做数乘。如果定义的这两种运算符合以下8条运算法则：对任意 $a,b,c\in V$ ，任意的 $k,l\in F$ ，都有 </p>
<ol>
<li>加法交换律：$a+b = b+c$ </li>
<li>加法结合律：$(a+b)+c=a+(b+c)$</li>
<li>零元：存在零元 $\theta\in V$ 使得 $a+\theta = a$</li>
<li>负元：存在 $a$ 的负元 $a’\in a$ 使得 $a+a’=\theta$</li>
<li>乘法结合律：$k(la)=(kl)a$</li>
<li>乘法分配律：<ol>
<li>$k(a+b)=ka+kb$</li>
<li>$(k+l)a=ka+la$</li>
</ol>
</li>
<li>乘法单位律：$1\cdot a=a$</li>
</ol>
<p>例1：集合 $V={\boldsymbol x|\boldsymbol x=[x_1,x_2,1]^T,\ x_1,x_2\in R}$ 不是一个向量空间，其（普通向量含义下的）加法和数乘法都<strong>不封闭</strong>，不符合定义。</p>
<script type="math/tex; mode=display">
\boldsymbol x=[1,1,1]^T \\
2\boldsymbol x = \boldsymbol x + \boldsymbol x = [2,2,2]^T \notin V</script><p>但可以定义新的加法和数乘，使其符合加法和数乘的条件和8条定律，如下，</p>
<script type="math/tex; mode=display">
v^{(1)}\oplus v^{(2)} = [x_1^{(1)}+x_1^{(2)},x_2^{(1)}+x_2^{(2)},1]\in V\\
k\otimes v = [kx_1,kx_2,1]\in V</script><p>例2：定义的加法和数乘也可以完全不符合通常的加法和乘法，比如 $a\oplus b = ab,\ k\otimes b=a^k$ </p>
<hr>
<h4 id="基和维数"><a href="#基和维数" class="headerlink" title="基和维数"></a>基和维数</h4><p>定义：如果给定域 $F$ 上的线性空间 $V$ 中存在一组向量 $\bm \alpha_1,…,\bm \alpha_r$ ，满足：</p>
<ol>
<li>$\bm \alpha_1,…,\bm \alpha_r$ 线性无关；</li>
<li>$V$ 中任意向量 $\bm \alpha$ 都能由线性 $\bm \alpha_1,…,\bm \alpha_r$ 表示，即存在唯一  $\lambda_1,…,\lambda_r\in F$ ，使得</li>
</ol>
<script type="math/tex; mode=display">
\boldsymbol \alpha = \lambda_1 \boldsymbol \alpha_1+...+\lambda_r\boldsymbol \alpha_r</script><p>则称 $\bm \alpha_1,…,\boldsymbol \alpha_r$ 为 $V$ 的一个<strong>基</strong>，系数 $\lambda_1,…,\lambda_r$ 称为 在此 $\boldsymbol \alpha$ 基下的<strong>坐标</strong>，基中的向量个数 $r$ 称为 $V$ 的<strong>维数</strong>，记为 $dimV = r$ 。</p>
<hr>
<h4 id="基的变换"><a href="#基的变换" class="headerlink" title="基的变换"></a>基的变换</h4><p>定义：设 $\bm \alpha_1,…,\bm \alpha_r$ 和  $\bm \beta_1,…,\bm \beta_r$ 是 $r$ 维线性空间 $V$ 的两组基，则存在可逆矩阵 $P$ ，使得</p>
<script type="math/tex; mode=display">
(\bm \beta_1,...,\bm \beta_r) = (\bm \alpha_1,...,\bm \alpha_r)P</script><p>则称上式为基变换公式，$P$ 为过渡矩阵，即</p>
<script type="math/tex; mode=display">
P = (\bm \alpha_1,...,\bm \alpha_r)^{-1} (\bm \beta_1,...,\bm \beta_r)</script><p>对于在不同基下两个相同的向量 $y$，在不同的两个基表达下分别为 $x_1,x_2$</p>
<script type="math/tex; mode=display">
\begin{aligned}
y & =  (\bm \alpha_1,...,\bm \alpha_r)\bm x_1 \\
y & =  (\bm \beta_1,...,\bm \beta_r)\bm x_2 \\
\Rightarrow \bm x_1 & = (\bm \alpha_1,...,\bm \alpha_r)^{-1} (\bm \beta_1,...,\bm \beta_r)\bm x_2 \\
&= P\bm x_2
\end{aligned}</script><p>这就是坐标变换公式</p>
<p>⭐注意这里有两个相反，$P$ 的先后位置是反的，$P$ 变化方向也是反的</p>
<hr>
<h4 id="生成空间"><a href="#生成空间" class="headerlink" title="生成空间"></a>生成空间</h4><script type="math/tex; mode=display">
V =span \{S\}= span\{ \bm \alpha_1,...,\boldsymbol \alpha_r ,...\} = \{ \bm x= \boldsymbol \alpha = \lambda_1 \boldsymbol \alpha_1+...+\lambda_r\boldsymbol \alpha_r+...|\lambda_i\in F\}</script><p>其中 $S$ 是该向生成空间的生成集合，$V$ 的一个生成集合不必是 $V$ 的一组基，因此不必是线下无关的，但是对给定向量空间的极小生成集合一定是一组基。</p>
<hr>
<h2 id="向量空间"><a href="#向量空间" class="headerlink" title="向量空间"></a>向量空间</h2><p>从线性空间的定义可以看出，任何元素只要可以定义符合8条定律的代数运算，就可以成为线性空间的元素。而在过去学习的向量 $(x_1,…,x_N)^T$ 只是其中一种。</p>
<p>但在现代数学（以集合论为基础）中：线性空间中的元素就是向量，向量就是线性空间中的元素。所以线性空间和向量空间指的是一个东西，数学中的向量定义就是从线性空间出发的。</p>
<p>从工科的角度，为了方便理解，可以认为将以前学习的向量认为是狭义的，数学角度的向量是广义的，而广义的向量和狭义的向量是同构的。</p>
<hr>
<h4 id="同构"><a href="#同构" class="headerlink" title="同构"></a>同构</h4><p><img src="https://gitee.com/HonorWithPupil/img-bed/raw/master/Img/2021/10/function-mapping.svg" alt="一般、单射、满射和双射函数"></p>
<p>定义：设 $V_1,V_2$ 是数域 $F$ 上的两个线性空间，若存在双射（见上图）$f: V_1 \rightarrow V_2$ 满足：</p>
<script type="math/tex; mode=display">
\forall \alpha,\beta \in V_1 ,\forall k\in F\\
\begin{aligned}
f(\alpha+\beta) &= f(\alpha) + f(\beta)\\
f(k\alpha) &= kf(\alpha)
\end{aligned}</script><p> 则称 $V_1,V_2$ 同构，$f$ 称为同构映射，表示为 $V_1 \cong V_2$</p>
<p>例：数域 $F$ 上的 $n$ 维性空间 $V\cong F^n$ </p>
<p>根据定义，对于映射 $f: V\rightarrow F^n$ ，即</p>
<script type="math/tex; mode=display">
\alpha = x_1\alpha_1 + ... + x_n\alpha_n \in V \overset{f}{\rightarrow}x = 
\begin{pmatrix}
x_1\\
\vdots\\
x_n
\end{pmatrix} \in F^n</script><p>易于证明 $f$ 是同构映射</p>
<hr>
<h4 id="万物皆向量"><a href="#万物皆向量" class="headerlink" title="万物皆向量"></a>万物皆向量</h4><p>根据同构定义，可以得到以下结论：</p>
<p>数域 $F$ 上的任意两个有限维线性空间同构的<strong>充要条件</strong>是它门有相同的维数。</p>
<p>这说明，<strong>维数是有限维线性空间的唯一本质特征</strong>。$F^n$ 并不是 $n$ 维线性空间中的一个，而是所有 $n$ 维线性空间的代表。</p>
<p>关于向量的定义，可以分为以下三种：</p>
<ol>
<li>物理角度：从初到末的箭头，与标量相对</li>
<li>计算机角度：有序的数字列表</li>
<li>数学角度：加法和数乘有意义的任何东西</li>
</ol>
<p>今后，为了消除歧义，我将只使用第三种定义，并直接称呼线性空间为向量空间。同时在这个定义下，矩阵是一种向量，而矩阵又是由向量组成，所以这里有一个俄罗斯套娃的关系，深刻思考这种关系，也可以解释为何数学中只研究向量和矩阵，而不研究更高维的张量（因为在数学的视角里其本质是一样的）。</p>
<p>例：以下内容皆为向量空间</p>
<ol>
<li>所有 $m\times n$ 阶矩阵按照矩阵的加法和数乘，构成向量空间 $R^{m\times n}$</li>
<li>闭区间 $[a,b]$ 上的连续函数按照通常函数的加法和数乘，构成向量空间 $C[a,v]$</li>
<li>次数小于 $n$ 的多项式按照通常多项式的加法和数乘，构成向量空间 $P[x]_n$</li>
<li>齐次线性方程组 $A\bm x=0$ 的所有解的集合构成向量空间 $N(A) = { \bm x\in R^n|A\bm x=0}$ ，称为 $A\bm x=0$ 的解空间或者 $A$ 的零空间</li>
<li>所有 $A\bm x$ 的集合构成向量空间 $R(A) = { \bm y\in R^m|y = A\bm x}$ ，称为矩阵 $A$ 的列空间</li>
<li>……</li>
</ol>
]]></content>
      <categories>
        <category>MatrixTheory</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵论3：子空间和线性变换</title>
    <url>/2021/10/15/%E7%9F%A9%E9%98%B5%E8%AE%BA3%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2/</url>
    <content><![CDATA[<p>矩阵即变换</p>
<h2 id="线性变换定义"><a href="#线性变换定义" class="headerlink" title="线性变换定义"></a>线性变换定义</h2><p>设 $V_1,V_2$ 是数域 $F$ 上的线性空间，映射 $T: V_1\to V_2$ ，如果这个映射保持加法运算和数乘运算，即</p>
<script type="math/tex; mode=display">
\def\bm{\boldsymbol}
\forall \bm \alpha,\bm \beta \in V_1,k\in F
\begin{cases}
&T(\bm \alpha + \bm \beta) =T(\bm \alpha) + T(\bm \beta) \\\\
&T(k\bm\alpha)=kT(\bm \alpha) 
\end{cases}</script><p>则称 $T$ 为 $V_1$ 到 $V_2$ 上的线性映射。</p>
<p>特别的，当 $V_1=V_2=V$ 时，称 $T$ 为 $V$上的线性变换。</p>
<h5 id="例1：不是线性映射"><a href="#例1：不是线性映射" class="headerlink" title="例1：不是线性映射"></a>例1：不是线性映射</h5><script type="math/tex; mode=display">
T:\mathbb{R}^2\to\mathbb{R}^2\\
\pmatrix{x_1\\x_2} \mapsto \pmatrix{x_1x_2\\x_1+x_2}</script><h5 id="例2：是线性映射，不是线性变换"><a href="#例2：是线性映射，不是线性变换" class="headerlink" title="例2：是线性映射，不是线性变换"></a>例2：是线性映射，不是线性变换</h5><script type="math/tex; mode=display">
T:\mathbb{R}^3\to\mathbb{R}^2\\
\pmatrix{x_1\\x_2\\x_3} \mapsto \pmatrix{x_1+x_2\\x_3-x_2}</script><hr>
<h2 id="线性变换的矩阵表达"><a href="#线性变换的矩阵表达" class="headerlink" title="线性变换的矩阵表达"></a>线性变换的矩阵表达</h2><h3 id="矩阵即变换（映射）"><a href="#矩阵即变换（映射）" class="headerlink" title="矩阵即变换（映射）"></a>矩阵即变换（映射）</h3><p>很明显 $T(\bm x)=A_{m\times n}\bm x$ 是一个映射，且保持加法运算和数乘运算，所以矩阵是一种线性映射</p>
<h3 id="变换（映射）即矩阵"><a href="#变换（映射）即矩阵" class="headerlink" title="变换（映射）即矩阵"></a>变换（映射）即矩阵</h3><p>设 $\bm \alpha_1,…,\bm \alpha_n$ 是 $V_1$ 的一组基， $\bm \beta_1,…,\bm \beta_m$ 是 $V_2$ 的一组基，存在线性映射 $T: V_1\to V_2$ ，则</p>
<script type="math/tex; mode=display">
T(\bm \alpha_i) = a_{1i}\bm\beta_1+...+a_{mi}\bm\beta_m</script><p>定义符号</p>
<script type="math/tex; mode=display">
T(\bm \alpha_1,...,\bm \alpha_n) = (T(\bm\alpha_1),...,T(\bm\alpha_n))</script><p>从而有</p>
<script type="math/tex; mode=display">
T(\bm \alpha_1,...,\bm \alpha_n) = (\bm \beta_1,...,\bm \beta_m)A</script><p>其中 $A=[a<em>{ij}]</em>{m\times n}$</p>
<p>向量空间中的任何向量都是一组基的线性组合，因此确定了基的变换，就确定了其中任意一个向量的变换方式，所以线性映射可以用矩阵表示。</p>
<p>从而有对于任何 $\bm \alpha=(\bm \alpha_1,…,\bm \alpha_n)\bm x \in V_1$ ，通过线性映射 $T$ 得到的 $\bm \beta=(\bm \beta_1,…,\bm \beta_n)\bm y \in V_2$ </p>
<script type="math/tex; mode=display">
\begin{align}
T((\bm \alpha_1,...,\bm \alpha_n)\bm x) &= (\bm \beta_1,...,\bm \beta_m)A\bm x \\
& = (\bm \beta_1,...,\bm \beta_m)\bm y
\end{align}</script><script type="math/tex; mode=display">
\Rightarrow \bm y = A\bm x</script><p>当 $V_1=V_2$ ，即线性变换；不失一般性地假定 $(\bm \alpha_1,…,\bm \alpha_n) = I$ ，则 $T(x)=Ax$ ；而在证明过程中转置 $(\bm \alpha_1,…,\bm \alpha_n)$ ，则会得到 $T(x)=xA’$ ，即线性变换的左乘形式。</p>
<h3 id="例：几个简单的线性变换"><a href="#例：几个简单的线性变换" class="headerlink" title="例：几个简单的线性变换"></a>例：几个简单的线性变换</h3><h4 id="旋转变换：逆时针旋转角-theta"><a href="#旋转变换：逆时针旋转角-theta" class="headerlink" title="旋转变换：逆时针旋转角 $\theta$"></a>旋转变换：逆时针旋转角 $\theta$</h4><script type="math/tex; mode=display">
\bm y = 
\begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix} \bm x</script><h4 id="反射变换：关于-l-theta-对称"><a href="#反射变换：关于-l-theta-对称" class="headerlink" title="反射变换：关于 $l_\theta$ 对称"></a>反射变换：关于 $l_\theta$ 对称</h4><script type="math/tex; mode=display">
\bm y = 
\begin{pmatrix}
\cos2\theta & \sin2\theta \\
\sin2\theta & -\cos2\theta
\end{pmatrix} \bm x</script><h4 id="伸缩变换"><a href="#伸缩变换" class="headerlink" title="伸缩变换"></a>伸缩变换</h4><script type="math/tex; mode=display">
\bm y = 
\begin{pmatrix}
a & 0 \\
0 & b
\end{pmatrix} \bm x</script><h4 id="投影变换"><a href="#投影变换" class="headerlink" title="投影变换"></a>投影变换</h4><script type="math/tex; mode=display">
\bm y = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{pmatrix} \bm x</script><p>其中旋转变换、反射变换和伸缩变换可逆，是双射，即同构变换；而投影变换不可逆，是单射，即同态变换</p>
<hr>
<h2 id="线性变换的性质"><a href="#线性变换的性质" class="headerlink" title="线性变换的性质"></a>线性变换的性质</h2><h3 id="线性相关"><a href="#线性相关" class="headerlink" title="线性相关"></a>线性相关</h3><script type="math/tex; mode=display">
\def\bm{\boldsymbol}
\bm \alpha_1,...,\bm \alpha_n \text{ 线性相关} \Rightarrow T(\bm\alpha_1),...,T(\bm \alpha_n) \text{ 线性相关}</script><p>注意反向不成立，例如对于 $T(\bm x)=\theta \bm x$</p>
<h3 id="矩阵也是向量"><a href="#矩阵也是向量" class="headerlink" title="矩阵也是向量"></a>矩阵也是向量</h3><p> $L(V)$ （也就是 $R^{n\times n}$ ）表示 $V$ 上的所有线性变换的集合，则对任意 $T,T_1,T_2 \in L(V),\alpha \in V,k\in F$，定义符合矩阵运算的加法和数乘，</p>
<script type="math/tex; mode=display">
(T_1+T_2)(\alpha) \equiv T_1(\alpha) + T_2(\alpha) \\
(kT)(\alpha) \equiv kT(\alpha)</script><p>易于证明其 $L(V)$ 也是向量空间，也就是说，矩阵既是向量的线性变换形式，自身也是一种向量。</p>
<h3 id="基的变换，线性变换的表示矩阵如何改变"><a href="#基的变换，线性变换的表示矩阵如何改变" class="headerlink" title="基的变换，线性变换的表示矩阵如何改变"></a>基的变换，线性变换的表示矩阵如何改变</h3><p>设 $T$ 为 $n$ 维向量空间 $V$ 上的线性变换，对于 $V$ 的基 $\bm \alpha_1,…,\bm \alpha_n$ 和 $\bm \beta_1,…,\bm \beta_n$ 分别表示为 $A$ 和 $B$，并且存在过渡矩阵 $P$，</p>
<script type="math/tex; mode=display">
(\bm \beta_1,...,\bm \beta_n) = (\bm \alpha_1,...,\bm \alpha_n)P</script><p>则有，</p>
<script type="math/tex; mode=display">
\begin{align}
  &(\bm \beta_1,...,\bm \beta_n)B \\ \\
= &T(\bm \beta_1,...,\bm \beta_n) \\ \\
= &T[(\bm \alpha_1,...,\bm \alpha_n)P] \\ \\
= &(\bm \alpha_1,...,\bm \alpha_n)AP \\ \\
= &(\bm \beta_1,...,\bm \beta_n)P^{-1}AP\\ \\

\Rightarrow &B=P^{-1}AP
\end{align}</script><p>同一个线性变换，当基改变后，其矩阵表示也会改变，但它们是相似的，而相似变换矩阵就是线性空间中不同基间的过渡矩阵。</p>
]]></content>
      <categories>
        <category>MatrixTheory</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习: 最小二乘法</title>
    <url>/2021/11/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</url>
    <content><![CDATA[<p>最小二乘法的基本原理如下，针对线性回归模型 $\def\bm{\boldsymbol} y=\bm w^T\bm x$，寻找参数 $w$ 使得残差平方和最小，</p>
<script type="math/tex; mode=display">
\def\part{\partial}
\def\grad{\nabla}
\min_\bm w f(\bm w)=||X\bm w-\bm y||^2</script><h2 id="直接求解"><a href="#直接求解" class="headerlink" title="直接求解"></a>直接求解</h2><script type="math/tex; mode=display">
\begin{align}
\frac{\part f(\bm w)}{\part \bm w}&=\frac{\part(X\bm w-\bm y)^T(X\bm w-\bm y)}{\part \bm w} \\ \\
&= 2X^T(X\bm w-\bm y) = 0 \\ \\
\Rightarrow \ & \bm w=(X^TX)^{-1}X\bm y
\end{align}</script><p>又因为二阶导数半正定，所以这个解是极值点。</p>
<script type="math/tex; mode=display">
\frac{\part^2f(\bm w)}{\part\bm w^2} = 2X^TX\succeq0</script><p>注意 $X^TX$ 存在逆，这要求 $X$ 是列无关的，这个要求在梯度下降法的证明中也是需要的，若这个要求不符合，意味着存在不知唯一一个 $w$ 满足残差平方和最小。</p>
<p>由于当数据规模极其巨大的时候，计算 $X^TX$ 是不现实的，所以通常情况下会使用迭代法，而考虑到 $X^TX$ 是一个半正定矩阵，最小二乘问题只需要凸优化方法即可。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><script type="math/tex; mode=display">
\bm w_{i+1} = \bm w_i -\eta\grad f(\bm w_i)</script><p>其中 $\eta$ 为学习率，是一个可供选择的常数或者变量。</p>
<h3 id="引理-L-smooth-条件"><a href="#引理-L-smooth-条件" class="headerlink" title="引理 L-smooth 条件"></a>引理 L-smooth 条件</h3><p>若函数 $f(x)$ 对于任意两个$x,y$，存在一个常数 $L&gt;0$ 使得，</p>
<script type="math/tex; mode=display">
||\grad f(x)-\grad f(y)||\le L||x-y||</script><p>则称 $f(x)$ 符合 L-Lipschitz 条件。</p>
<p>证明：</p>
<script type="math/tex; mode=display">
\begin{align}
||\grad f(\bm w_1)-\grad f(\bm w_2)|| &= ||2X^T(X\bm w_1-\bm y)-2X^T(X\bm w_2-\bm y)|| \\ \\
& = 2||X^TX(\bm w_1-\bm w_2)|| \\ \\
& \le  ||X^TX||\cdot 2||\bm w_1-\bm w_2||
\end{align}</script><p>另 $L=2||X^TX||&gt;0$，所以函数 $f(\bm w)$ 符合  L-smooth 条件。</p>
<h3 id="引理-L-smooth-等价形式"><a href="#引理-L-smooth-等价形式" class="headerlink" title="引理 L-smooth 等价形式"></a>引理 L-smooth 等价形式</h3><script type="math/tex; mode=display">
|f(x)-f(y)-\grad f(y)^T(x-y)|\le \frac{L}{2}||x-y||^2</script><p>证明：</p>
<p>构造一个插值函数 $g(t) = f(y+t(x-y))$，对 $t$ 求导，</p>
<script type="math/tex; mode=display">
g'(t)=\grad f(y+t(x-y))^T(x-y)</script><p>可以把函数值之差转变为积分，</p>
<script type="math/tex; mode=display">
f(x)-f(y)=g(1)-g(0) =\int_0^1\grad f(y+t(x-y))^T(x-y)dt</script><p>将 上式代入等式左侧，</p>
<script type="math/tex; mode=display">
\begin{align}
\text{left} &= |\int_0^1\grad f(y+t(x-y))^T(x-y)dt-\grad f(y)^T(x-y)| \\ \\
& = |\int_0^1[\grad f(y+t(x-y))^T(x-y) - \grad f(y)^T(x-y)]dt| \\ \\
& \le \int_0^1|[\grad f(y+t(x-y))-\grad f(y)]^T(x-y)|dt \\ \\
& \le \int_0^1 \sqrt{||\grad f(y+t(x-y))-\grad f(y)||^2||x-y||^2} dt \\ \\
& \le \int_0^1 \sqrt{L||t(x-y)||^2||x-y||^2} dt \\ \\
& =L||x-y||^2\int_0^1tdt=\frac{L}{2}||x-y||^2
\end{align}</script><p>其中第3行使用了和的绝对值小于等于绝对值的和，第4行使用了柯西施瓦茨不等式 $a^Tb\le\sqrt{||a||^2||b||^2}$ ，第5行代入了 L-smooth 条件。</p>
<p>删去绝对值，可得到，</p>
<script type="math/tex; mode=display">
f(x)\le f(y)+\grad f(y)^T(x-y)+\frac{L}{2}||x-y||^2</script><h3 id="证明-收敛性（固定学习率）"><a href="#证明-收敛性（固定学习率）" class="headerlink" title="证明 收敛性（固定学习率）"></a>证明 收敛性（固定学习率）</h3><p>接下来证明梯度下降法在最小二乘问题上的收敛性，注意这个证明具有一般性（符合 L-smooth 条件的凸函数），所以会略显冗长。</p>
<script type="math/tex; mode=display">
\begin{align}
f(\bm w_{i+1}) &\le f(\bm w_i) +\grad f(\bm w_i)^T(\bm w_{i+1}-\bm w)+\frac{L}{2}||\bm w_{i+1}-\bm w||^2 \\ \\
& = f(\bm w_i) +\grad f(\bm w_i)^T(-\eta\grad f(\bm w_i))+\frac{L}{2}||-\eta\grad f(\bm w_i)||^2 \\ \\ 
& = f(\bm w_i)-(1-\frac{L\eta}{2})\eta||\grad f(\bm w_i)||^2
\end{align}</script><p>选择 $\eta L\le 1$ ，则 $1-\frac{L\eta}{2}\ge\frac{1}{2}$ ，所以得到：</p>
<script type="math/tex; mode=display">
f(\bm w_{i+1})\le f(\bm w_i) - \frac 1 2\eta||\grad f(\bm w_i)||^2</script><p>观察上面的公式，我们会发现 $f(\bm w)$ 每次迭代，都会让 $f(\bm w)$ 变得更小，朝着更好的方向去前进，也就是单调性。接下来我们继续证明收敛性。</p>
<p>假设最优解 $f(\bm w^*)$ 为最优解，那么根据泰勒一阶展开，以及 $f(\bm w)$ 是一个凸函数：</p>
<script type="math/tex; mode=display">
f(\bm w_i)\le f(\bm w^*)+\grad f(\bm w_i)^T(\bm w_i-\bm w^*)</script><p>代入上式子，</p>
<script type="math/tex; mode=display">
\begin{align}
f(\bm w_{i+1}) &\le f(\bm w^*)+\grad f(\bm w_i)^T(\bm w_i-\bm w^*) - \frac 1 2\eta||\grad f(\bm w_i)||^2 \\ \\
f(\bm w_{i+1}) - f(\bm w^*) &\le \frac{1}{2\eta}\left( 2\eta\grad f(\bm w_i)^T(\bm w_i -\bm w^*)-\eta^2||\grad f(\bm w_i)||^2  \right) \\ \\
f(\bm w_{i+1}) - f(\bm w^*) &\le \frac{1}{2\eta}\left( ...-||\bm w_i -\bm w^*||^2+||\bm w_i -\bm w^*||^2  \right) \\ \\
f(\bm w_{i+1}) - f(\bm w^*) &\le \frac{1}{2\eta}\left( ||\bm w_i -\bm w^*||^2-||\bm w_i - \eta\grad f(\bm w_i) -\bm w^*||^2  \right) \\ \\
f(\bm w_{i+1}) - f(\bm w^*) &\le \frac{1}{2\eta}\left( ||\bm w_i -\bm w^*||^2-||\bm w_{i+1} -\bm w^*||^2  \right)
\end{align}</script><p>将 $i=0,…,k-1$ 代入上式，得到</p>
<script type="math/tex; mode=display">
\begin{align}
f(\bm w_{1}) - f(\bm w^*) &\le \frac{1}{2\eta}\left( ||\bm w_0 -\bm w^*||^2-||\bm w_1 -\bm w^*||^2  \right) \\ \\
f(\bm w_2) - f(\bm w^*) &\le \frac{1}{2\eta}\left( ||\bm w_1 -\bm w^*||^2-||\bm w_2 -\bm w^*||^2  \right) \\ \\
&\vdots \\ \\
f(\bm w_{k}) - f(\bm w^*) &\le \frac{1}{2\eta}\left( ||\bm w_{k-1} -\bm w^*||^2-||\bm w_{k} -\bm w^*||^2  \right) 
\end{align}</script><p>将上式全部相加，得到</p>
<script type="math/tex; mode=display">
\begin{align}
\sum_{i=1}^k[f(\bm w_{i}) - f(\bm w^*)] &= \frac{1}{2\eta}(\left( ||\bm w_0 -\bm w^*||^2-||\bm w_k -\bm w^*||^2  \right)) \\ \\
&\le \frac{1}{2\eta}||\bm w_0 -\bm w^*||^2 \\ \\
\sum_{i=1}^k[f(\bm w_k) - f(\bm w^*)] &\le \sum_{i=1}^k[f(\bm w_i) - f(\bm w^*)] \le \frac{1}{2\eta}||\bm w_0 -\bm w^*||^2 \\ \\
f(\bm w_k) &\le f(\bm w^*)+\frac{1}{2\eta k}||\bm w_0 -\bm w^*||^2
\end{align}</script><p>证明完毕。随着 $k$ 越来越大，误差 $\epsilon=\frac{1}{2\eta k}||\bm w_0 -\bm w^*||^2$ 也越来越小，从上面得到的公式我们可以知道在一个符合 L-smooth 的凸函数上，梯度下降法的收敛步数为 $O(\frac{1}{\epsilon})$ ，是次线性收敛 ；若在证明中加入强凸的属性，则梯度下降法的收敛步数为 $O(log(\frac{1}{\epsilon}))$ ，是线性收敛。</p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>由泰勒展开式得，</p>
<script type="math/tex; mode=display">
f(\bm w_i +\bm d) = f(\bm w_i)+\grad f(\bm w_i)^T\bm d + \frac{1}{2}\bm d^T\grad^2f(\bm w_i)\bm d + o(||\bm d||^2)</script><p>舍弃高阶项，并对 $\bm d$ 求导，</p>
<script type="math/tex; mode=display">
\grad_{\bm d}f(\bm w_i +\bm d) =\grad f(\bm w_i)+\grad^2f(\bm w_i)\bm d=0 \\ \\
\Rightarrow \bm d = -\grad^2f(\bm w_i)^{-1}\grad f(\bm w_i)</script><p>因此迭代方程为，</p>
<script type="math/tex; mode=display">
\bm w_{i+1} = \bm w_i-\grad^2f(\bm w_i)^{-1}\grad f(\bm w_i)</script><h3 id="证明-收敛性"><a href="#证明-收敛性" class="headerlink" title="证明 收敛性"></a>证明 收敛性</h3><p>首先对于最小二乘问题，</p>
<script type="math/tex; mode=display">
H(\bm w) = \grad^2f(\bm w) = 2X^TX=C</script><p>在迭代方程两边同时减去最优点 $\bm w^*$ ，</p>
<script type="math/tex; mode=display">
\begin{align}
\bm w_{i+1} - \bm w^* &= \bm w_i - \bm w^* -H^{-1}(\bm w_i)\grad f(\bm w_i) \\ \\
& = \bm w_i - \bm w^* -H^{-1}(\bm w_i)[\grad f(\bm w_i)-\grad f(\bm w^*)]\ \ (\grad f(\bm w^*)=\bm 0)
\end{align}</script><p>构造插值函数 </p>
<script type="math/tex; mode=display">
g(t)=\grad f(\bm w_i+t(\bm w^*-\bm w_i))</script><p>则</p>
<script type="math/tex; mode=display">
g'(t)=H(\bm w_i+t(\bm w^*-\bm w_i))(\bm w^*-\bm w_i)</script><script type="math/tex; mode=display">
\begin{align}
\grad f(\bm w^*)-\grad f(\bm w_i) &= g(1)-g(0) \\ \\
&= \int_0^1g'(t)dt \\ \\
-\grad f(\bm w_i)&= \int_0^1H(\bm w_i+t(\bm w^*-\bm w_i))(\bm w^*-\bm w_i)dt
\end{align}</script><p>故得，</p>
<script type="math/tex; mode=display">
\begin{align}
\bm w_{i+1} - \bm w^* &= \bm w_i - \bm w^* + H^{-1}(\bm w_i)\int_0^1H(\bm w_i+t(\bm w^*-\bm w_i))(\bm w^*-\bm w_i)dt \\ \\
& = H^{-1}(\bm w_i)\int_0^1[H(\bm w_i+t(\bm w^*-\bm w_i))-H(\bm w_i)](\bm w^*-\bm w_i)dt \\ \\
&= 0
\end{align}</script><p>因此对于最小二乘问题（线性），对于任意起点，牛顿法只需要一步就可以达到最优点。对于没有海森矩阵 $H(w)$ 为常数性质的问题，当海森矩阵有界且 Lipschitz 连续时，也可以通过一系列范数放缩，证明牛顿法是二次收敛的。</p>
<p>其实对于最小二乘法问题（线性）牛顿法和直接求解法是相同的。</p>
<script type="math/tex; mode=display">
\begin{align}
\bm w_{i+1} &= \bm w_i-\grad^2f(\bm w_i)^{-1}\grad f(\bm w_i) \\ \\
&= \bm w_i - (2X^TX)^{-1}\cdot2X^T(X\bm w_i-\bm y) \\ \\
&= (X^TX)^{-1}X\bm y
\end{align}</script><p>可见直接使用牛顿法，在最小二乘问题上，其实是意义不大的。</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>optimal</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习: AdaBoost</title>
    <url>/2021/11/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0-AdaBoost/</url>
    <content><![CDATA[<p>对于一个二分类问题数据集</p>
<script type="math/tex; mode=display">
T = \lbrace (x_1,y_1),...,(x_N,y_N) \rbrace</script><p>其中 $x_i\in \mathcal X \subseteq \mathbb R^m, y_i\in\mathcal Y=\lbrace-1,+1\rbrace$ 。AdaBoost 使用以下算法，从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合称为一个强分类器。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>输入：训练集数据；弱学习器 $G_m$ </p>
<p>输出：一个强学习器 $G$</p>
<h4 id="1-初始化训练数据的权重向量"><a href="#1-初始化训练数据的权重向量" class="headerlink" title="1. 初始化训练数据的权重向量"></a>1. 初始化训练数据的权重向量</h4><script type="math/tex; mode=display">
D_1=(\overbrace{\frac{1}{N},...,\frac{1}{N}}^N)</script><h4 id="2-for-m-1-…-M"><a href="#2-for-m-1-…-M" class="headerlink" title="2. for m=1,…,M"></a>2. for m=1,…,M</h4><p>a）使用具有权重 $D_m$ 的数据集训练弱学习器，得到</p>
<script type="math/tex; mode=display">
G_m(x):\mathcal X\to \lbrace-1,+1\rbrace</script><p>b）计算 $G_m(x)$ 在训练集上的分类误差率，</p>
<script type="math/tex; mode=display">
e_m=\sum_{n=1}^N w_{mn}I(G_m(x_n)\neq y_n)</script><p>c）更新权重向量，</p>
<script type="math/tex; mode=display">
w_{m+1,n} = \frac{w_{mn}}{Z_m} \exp(-\alpha_my_n G_m(x_n)) \\ \\
Z_m = \sum_{n=1}^N w_{mn}\exp(-\alpha_my_n G_m(x_n)) \\ \\
\alpha_m = \frac 1 2 \ln \frac{1-e_m}{e_m}</script><p>其中 $\alpha_m$ 是最重要的参数，它不但表示在最终分类器中，每一个弱分类器所占的比重，也表示每个弱分类器对权重更新的程度。</p>
<h4 id="3-构建最终学习器"><a href="#3-构建最终学习器" class="headerlink" title="3. 构建最终学习器"></a>3. 构建最终学习器</h4><script type="math/tex; mode=display">
G(x)=sign(\sum_{m=1}^M\alpha_mG_m(x))</script><h2 id="误差上界"><a href="#误差上界" class="headerlink" title="误差上界"></a>误差上界</h2><script type="math/tex; mode=display">
e(T) = \frac 1 N \sum_{n=1}^N I(G(x_n)\neq y_n) \le \prod_{m=1}^M Z_m</script><p>证明：</p>
<p>首先易得，</p>
<script type="math/tex; mode=display">
\begin{align}
e(T) &= \frac 1 N \sum_{n=1}^N I(G(x_n)\neq y_n) \\ \\ 
&\le \frac 1 N \sum_{n=1}^N \exp\left[-y_n\sum_{m=1}^M\alpha_mG_m(x_n)\right] \\ \\
& = \sum_{n=1}^N w_{1n}\prod_{m=1}^M \exp\left(-y_n\alpha_mG_m(x_n)\right)
\end{align}</script><p>由权重更新公式得到，</p>
<script type="math/tex; mode=display">
Z_m w_{m+1,n} = w_{mn} \exp(-\alpha_m y_n G_m(x_n))</script><p> 继续推导，</p>
<script type="math/tex; mode=display">
\begin{align}
e(T) 
&\le \frac 1 N \sum_{n=1}^N \exp\left[-y_n\sum_{m=1}^M\alpha_mG_m(x_n)\right] \\ \\
& = \sum_{n=1}^N w_{1n}\prod_{m=1}^M \exp(-\alpha_my_nG_m(x_n)) \\ \\
& = \sum_{n=1}^N w_{1n}\exp(-\alpha_1y_nG_1(x_n)) \prod_{m=2}^M \exp(-\alpha_my_nG_m(x_n)) \\ \\
& = \sum_{n=1}^N Z_1w_{2n} \prod_{m=2}^M \exp(-\alpha_my_nG_m(x_n)) \\ \\
& = Z_1\sum_{n=1}^N w_{2n}\exp(-\alpha_2y_nG_2(x_n)) \prod_{m=3}^M \exp(-\alpha_my_nG_m(x_n)) \\ \\
& = \ \cdots \\ \\
& = \prod_{m=1}^M Z_m
\end{align}</script><h2 id="alpha-m-的选择"><a href="#alpha-m-的选择" class="headerlink" title="$\alpha_m$ 的选择"></a>$\alpha_m$ 的选择</h2><p>我们现在已经知道了误差上界为 $\prod_{m=1}^M Z_m$ ，因此应该寻找 $\alpha_m$ 使得 $Z_m$ 最小，</p>
<script type="math/tex; mode=display">
\def\part{\partial}
\begin{align}
\frac{\part Z_m}{\part \alpha_m}
&= \frac{\part \sum_{n=1}^N w_{mn}\exp(-\alpha_my_n G_m(x_n))}{\part \alpha_m} \\ \\
&= -\sum_{n=1}^Nw_{mn}y_n G_m(x_n)\exp(-\alpha_my_n G_m(x_n)) \\ \\
& = -\exp(-\alpha_m)\sum_{y_i=G_m(x_i)}w_{mn}+\exp(\alpha_m)\sum_{y_i\neq G_m(x_i)}w_{mn} \\ \\
& = -\exp(-\alpha_m)(1-e_m)+\exp(\alpha_m)e_m = 0 \\ \\
&\ \ \Rightarrow \alpha_m=\frac 1 2 \ln(\frac{1-e_m}{e_m})
\end{align}</script><p> 确定完 $\alpha_m$ 之后，我们可以进一步界定误差上界，</p>
<script type="math/tex; mode=display">
\begin{align}
Z_m &= \exp(-\alpha_m) \sum_{y_i=G_m(x_i)} w_{mn} + \exp(\alpha_m) \sum_{y_i\neq G_m(x_i)} w_{mn} \\ \\
&= (1-e_m)\sqrt{\frac{e_m}{1-e_m}}+e_m\sqrt{\frac{1-e_m}{e_m}} \\ \\
&=2\sqrt{(1-e_m)e_m} \le 1
\end{align}</script><p>因此，只要保证每个弱分类器只要比随机猜测略好（$e_m&lt;0.5$），就能保证最终的强分类器的误差上界被不断减小。但这个减小的程度是有限的，随着无法找到不同的弱分类器，就无法再继续了。</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵论4：子空间、值域与核、不变子空间</title>
    <url>/2021/11/10/%E7%9F%A9%E9%98%B5%E8%AE%BA4%EF%BC%9A%E5%AD%90%E7%A9%BA%E9%97%B4%E3%80%81%E5%80%BC%E5%9F%9F%E4%B8%8E%E6%A0%B8%E3%80%81%E4%B8%8D%E5%8F%98%E5%AD%90%E7%A9%BA%E9%97%B4%20-%20%E5%89%AF%E6%9C%AC/</url>
    <content><![CDATA[<p>本章介绍一些之前几篇文章为了逻辑顺畅暂且搁置的知识。</p>
<h2 id="子空间"><a href="#子空间" class="headerlink" title="子空间"></a>子空间</h2><p>在介绍本章的主角线性空间之前，我们要先铺垫一下子空间的概念。</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>定义：设有线性空间 $\def\bm{\boldsymbol} V_1$ 及 $V_2$ 。如果 $V_1\subseteq V_2$，则称  $V_1$ 是 $V_2$ 的子空间。</p>
<p>例：$V$ 和 ${\bm\theta}$ 是 $V$ 的子空间，称它们为平凡子空间，其余的为非平凡子空间。</p>
<p>例：集合 $\lbrace T=\boldsymbol x|\boldsymbol x=[x_1,x_2,0]^T,\ x_1,x_2\in R \rbrace$ 是一个向量空间，它是 $R^3$ 中的向量 $[x_1,x_2,x_3]^T$ 在 $x_1x_2$ 平面上的投影空间，是 $R^3$ 的一个子空间。</p>
<p>例：$R^3$ 不是 $R^4$ 的子空间。</p>
<h4 id="定理：子空间判别法"><a href="#定理：子空间判别法" class="headerlink" title="定理：子空间判别法"></a>定理：子空间判别法</h4><p>数域 $F$ 上的向量空间 $V$ 的非空子集 $U$ 是向量空间（ $V$ 的子空间 ）的充要条件是 $U$ 对 $V$ 中规定的加法和数乘都封闭。</p>
<p>⭐ 这意味着子空间只要对原空间的两个运算封闭即可，</p>
<h4 id="交与和"><a href="#交与和" class="headerlink" title="交与和"></a>交与和</h4><p>定理（交集）：设 $V_1,V_2$ 是数域 $F$ 上的向量空间 $V$ 的两个子空间，则它们的交集 $V_1\cap V_2$ 也是 $V$ 的子空间。</p>
<p>定理（和）：设 $V_1,V_2$ 是数域 $F$ 上的向量空间 $V$ 的两个子空间，定义 $V_1,V_2$ 的和如下，</p>
<script type="math/tex; mode=display">
\def\bm{\boldsymbol} \def\span{\text{span}}
\begin{align}
V_1+V_2 &\overset{\underset{\mathrm{def}}{}}{=} \{ \bm\alpha+\bm\beta|\bm\alpha\in V_1,\bm\beta \in V_2 \} 
\\ \\
&= \span(\bm\alpha_1,\dots,\bm\alpha_s,\bm\beta_1,\dots,\bm\beta_t)
\\ \\
\text{where   } &V_1 = \span(\bm\alpha_1,\dots,\bm\alpha_s)
\\ \\
&V_2 = \span(\bm\beta_1,\dots,\bm\beta_t)
\end{align}</script><p>其也是 $V$ 的子空间。</p>
<p>⭐问题：为何不研究子空间的并集</p>
<p>反例：由三维空间的 $xy$ 平面和 $yz$ 平面给组成的并集，并不对加法封闭。</p>
<p>本质：交与和 是对子空间的基进行交和并运算（用线性相关替代重复的意义下），而不是对于集合进行交和并运算，恰巧集合的交和基的交相同，而集合的并和基的并不相同</p>
<h4 id="定理：维数公式"><a href="#定理：维数公式" class="headerlink" title="定理：维数公式"></a>定理：维数公式</h4><p>设 $V_1,V_2$ 是数域 $F$ 上的向量空间 $V$ 的两个有限维子空间，则它们的 交 和 和 都是有限维的，并且</p>
<script type="math/tex; mode=display">
\def\dim{\text{dim}}
\dim(V_1+V_2) = \dim(V_1)+\dim(V_2)-dim(V_1\cap V_2)</script><h4 id="直和（direct-sum）"><a href="#直和（direct-sum）" class="headerlink" title="直和（direct sum）"></a>直和（direct sum）</h4><p>设 $V_1,V_2$ 是数域 $F$ 上的向量空间 $V$ 的两个子空间，如果</p>
<script type="math/tex; mode=display">
V_1\cap V_2 = \{ \theta \}</script><p>则称 $V_1+V_2$ 为 $V_1,V_2$ 的直和，记作 $V_1\oplus V_2$ 。</p>
<h4 id="定理：直和分解"><a href="#定理：直和分解" class="headerlink" title="定理：直和分解"></a>定理：直和分解</h4><p>设 $V_1$ 是数域 $F$ 上的线性空间 $V$ 的一个子空间，则一定存在 $V$ 的另一个子空间 $V_2$（仅当 $V_1 = {\theta }$ 时唯一，为 $V$），使得空间 $V$ 具有直和分解，</p>
<script type="math/tex; mode=display">
V = V_1 \oplus V_2</script><p>并称 $V_1,V_2$ 是一对互补的子空间，或者 $V_1$ 是 $V_2$ 的补子空间。</p>
<p>⭐ 直和分解不唯一</p>
<p>⭐ 三个子空间两两直和，其不一定直和 $ \Leftrightarrow $ 三个向量两两线性无关，其不一定线性无关</p>
<hr>
<h2 id="值域与核"><a href="#值域与核" class="headerlink" title="值域与核"></a>值域与核</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>若有线性映射 $T:V_1\mapsto V_2$，</p>
<p>线性映射 $T$ 的值域：</p>
<script type="math/tex; mode=display">
\def\Im{\text{Im}} 
\def\Ker{\text{Ker}}
\def\bm{\boldsymbol}
\Im(T)=R(T)=\{T(\bm \alpha)|\forall \bm \alpha \in V_1\}</script><p>线性映射 $T$ 的核：</p>
<script type="math/tex; mode=display">
\Ker(T)=N(T)=\{ \bm \alpha\in V_1|T(\bm \alpha)=\bm \theta_2 \}</script><p>⭐ 注意  $\Im(T)$ 是 $V_2$ 的子空间；$\Ker(T)$ 是 $V_1$ 的子空间</p>
<p>⭐ $dim(\Im(T))$ 是经过变换最多保留的维度；$dim(\Ker(T))$ 是经过变换的舍弃的维度，所以</p>
<script type="math/tex; mode=display">
dim(\Im(T))+dim(\Ker(T)) = dim(V_1)</script><p>⭐ $dim(V_2)&gt;dim(\Im(T))$ 即可，可以任意大</p>
<p>当 $V_1=V_2$ ，$T$ 为线性变换时， $\Im(T)$ 的维数即线性变换 $T$ 的秩； $\Ker(T)$ 的维数即线性变换 $T$ 的零度。</p>
<script type="math/tex; mode=display">
rank(T)+nullity(T) = n</script><p>但一般情况下，</p>
<script type="math/tex; mode=display">
\Im(T)+\Ker(T)\ne V</script><hr>
<h2 id="不变子空间"><a href="#不变子空间" class="headerlink" title="不变子空间"></a>不变子空间</h2><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><p>设 $T$ 是数域 $F$ 上的线性空间 $V$ 上的线性变换，$W$ 是 $V$ 的子空间。如果对任意向量 $\bm \alpha\in W$ 都有 $T(\bm \alpha)\in W$ ，则称 $W$ 是 $T$ 的不变子空间。并且称 $T|_W:W\to W$ 为 $T$ 在 $W$ 上的限制。</p>
<p>例1： ${ \bm \theta } $ 是任意线性空间任意线性变换的不变子空间，因为 $T(\bm\theta)=\bm\theta$ 。</p>
<p>例2： $\Im(T)$ 是 $T$ 的不变子空间，由值域定义显然。</p>
<p>例3： $\Ker(T)$ 是 $T$ 的不变子空间，因为对任意向量 $\bm \alpha\in \Ker(T)$ 都有 $T(\bm \alpha)=\bm \theta$，且 $\bm\theta\in \Ker(T)$ 。</p>
<p>例4： $T$ 的特征子空间</p>
<script type="math/tex; mode=display">
V_\lambda=\{ \bm x|T(\bm x)=\lambda \bm x, \bm x \in V \}</script><p>也是 $T$ 的不变子空间。</p>
<h3 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h3><h5 id="定理-1："><a href="#定理-1：" class="headerlink" title="定理 1："></a>定理 1：</h5><p>线性变换 $T$ 的不变空间的交和和仍然是 $T$ 的不变子空间。</p>
<h5 id="定理-2："><a href="#定理-2：" class="headerlink" title="定理 2："></a>定理 2：</h5><p>线性空间 $V$ 上的线性变换 $T$ 有非平凡的不变子空间的充要条件是 $T$ 在 $V$ 的一组基下的矩阵表示为（相似于）块上三角矩阵，即形如，</p>
<script type="math/tex; mode=display">
\pmatrix{A_{11}&A_{12}\\ O&A_{22}}</script><p>对应的不变子空间形如 $(x_1,\dots,x_m,0,\dots,0)$ 。</p>
<p>其中 $m$ 为 $dim(A_{11})$。</p>
<h5 id="定理-3："><a href="#定理-3：" class="headerlink" title="定理 3："></a>定理 3：</h5><p>线性空间 $V$ 上的线性变换 $T$ 在 $V$ 的一组基下的矩阵表达式为块对角矩阵 $diag\lbrace A_1,A_2,…,A_s\rbrace$ 的充要条件是 $V$ 可以分解为 $T$ 的不变子空间的直和</p>
<script type="math/tex; mode=display">
V=W_1\oplus W_2\oplus ...\oplus W_s</script><p>若 $T$ 可对角化，本定理的块对角矩阵即可变为对角矩阵，其中</p>
<script type="math/tex; mode=display">
A_i=\lambda_i \\
W_i = V_{\lambda_i}=\lbrace \bm x|T(\bm x)=\lambda_i \bm x, \bm x \in V \rbrace</script><p>这个定理既可以求不变子空间，也为下一节中的约当型做出了铺垫。</p>
<h2 id="考点"><a href="#考点" class="headerlink" title="考点"></a>考点</h2><ol>
<li>子空间的判别；</li>
<li>求不变子空间。</li>
</ol>
]]></content>
      <categories>
        <category>MatrixTheory</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵论5：Jordan标准型</title>
    <url>/2021/12/10/%E7%9F%A9%E9%98%B5%E8%AE%BA5%EF%BC%9AJordan%E6%A0%87%E5%87%86%E5%9E%8B/</url>
    <content><![CDATA[<blockquote>
<p>Jordan 标准形理论，也即方阵在相似下的分类理论，可以说是线性代数的最深刻的地方。   </p>
<p>—— 李炯生 《线性代数》</p>
</blockquote>
<p>由于一般矩阵未必与对角矩阵相似，因此我们“退而求其次“，寻找几乎对角的矩阵，这也就是 $Jordan$ 标准型。</p>
<p><strong>定理</strong> 对于任意非零矩阵 $A$，存在可逆矩阵 $P$，使得 $P^{-1}AP=J$ ， $J$ 是 $A$ 的 $Jordan$ 标准型</p>
<script type="math/tex; mode=display">
J=\text{diag}(J_{e_1}(\lambda_1),...,J_{e_k}(\lambda_s))</script><p>$\lambda<em>i$ 为 $A$ 的特征值 ， $J</em>{e_j}(\lambda_i)$ 为 $Jordan$ 块，形如，</p>
<script type="math/tex; mode=display">
J_{e_j}(\lambda_i)=
\pmatrix{
\lambda_i & 1 & & \\
 & \lambda_i & \ddots & \\
 &  & \ddots  & 1 \\
 &  & & \lambda_i \\
}_{e_j\times e_j}</script><p>并且在不考虑对角块顺序的情况下，$J$ 是唯一的。</p>
<h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><p>由于能力和时间有限，不给出 $Jordan$ 标准型定理的完整证明，仅介绍其中两个核心定理，即空间第一分解定理和空间第二分解定理，以便于更深层地理解 $Jordan$ 标准型原理。</p>
<h3 id="空间第一分解定理"><a href="#空间第一分解定理" class="headerlink" title="空间第一分解定理"></a>空间第一分解定理</h3><p>设 $V$ 是 $n$ 维复线性空间，$A:V\to V$ 是线性变换，$\lambda$ 是其特征值，记</p>
<script type="math/tex; mode=display">
\def\Ker{\text{Ker}}
\def\Im{\text{Im}}
W^m_\lambda = \Ker(A-\lambda I)^m</script><p>由值域的定义 $(A-\lambda I)^m\alpha=0$ ，显然可得，</p>
<script type="math/tex; mode=display">
W^1_\lambda\subseteq W^2_\lambda\subseteq...\subseteq W^m_\lambda\subseteq...</script><p>由于 $V$ 是有限维空间，显然 $\dim W^m_\lambda$ 的维数无法无限增长，易于得到以下结论，存在 $k$ 使得</p>
<script type="math/tex; mode=display">
...\subset W_\lambda^{k-1}\subset W_\lambda^k=W_\lambda^{k+1}=...</script><p>同时易于证明 $W^k_\lambda$ 是不变子空间</p>
<h4 id="根子空间定义"><a href="#根子空间定义" class="headerlink" title="根子空间定义"></a>根子空间定义</h4><p>设 $\lambda$ 是线性变换 $A:V\to V$ 的特征值，则所有 $N_m(\lambda)$ 的并集称为线性变换 $A$ 的属于特征值 $\lambda$ 的根子空间，记作 $N(\lambda)$。</p>
<script type="math/tex; mode=display">
W_\lambda :=\bigcup_{m=1}^\infty W^m_\lambda=W^k_\lambda</script><h4 id="空间第一分解定理-1"><a href="#空间第一分解定理-1" class="headerlink" title="空间第一分解定理"></a>空间第一分解定理</h4><p>设 $\lambda_1,…,\lambda_t$ 是线性变换 $A:V\to V$ 的全部不同特征值，它们的代数重数分别为 $e_1,…e_t$ ，即 $A$ 的特征多项式</p>
<script type="math/tex; mode=display">
\varphi(\lambda)=(\lambda-\lambda_1)^{e_1}...(\lambda-\lambda_t)^{e_t}</script><p>则有</p>
<script type="math/tex; mode=display">
V=W_{\lambda_1}\oplus...\oplus W_{\lambda_t}</script><p>证明略。</p>
<h3 id="空间第二分解定理"><a href="#空间第二分解定理" class="headerlink" title="空间第二分解定理"></a>空间第二分解定理</h3><h4 id="循环子空间"><a href="#循环子空间" class="headerlink" title="循环子空间"></a>循环子空间</h4><p>设 $\alpha_0$ 是 $V$ 的非零向量， $V$ 中的线性变换 $A$ 的所有包含 $\alpha_0$ 的不变子空间的交称为由向量 $\alpha_0$ 生成的（相对线性变换 $A$ 的）循环子空间，记为 $C_0$ ，并且可以证明</p>
<script type="math/tex; mode=display">
C_0=\text{span}\lbrace \alpha_0,A\alpha_0,...,A^m\alpha_0,... \rbrace</script><h4 id="空间第二分解定理-1"><a href="#空间第二分解定理-1" class="headerlink" title="空间第二分解定理"></a>空间第二分解定理</h4><p>设 $W_{\lambda_j}$ 是线性变换 $A:V\to V$ 的属于特征值 $\lambda_j$ 的根子空间，则</p>
<script type="math/tex; mode=display">
W_{\lambda_j} = C_{j1}\oplus C_{j2} \oplus ...\oplus C_{jk_j}</script><p>其中 $C<em>{jl}$ 是相对线性变换 $(A-\lambda_jI)|</em>{W_{\lambda_j}}$ 的循环子空间，其中 $l=1,2,…,k_j$ ，$j=1,2,…,t$ 。</p>
<p>证明略。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过空间第一、第二分解定律，可以将 $V$ 分解为根子空间（对映不同特征值），每个根子空间再分解为循环子空间（对映集合重数），对应到 $J$ 中的每个 $Jordan$ 块。</p>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><script type="math/tex; mode=display">
A=\begin{pmatrix}
3&-4&0&2\\
4&-5&-2&4\\
0&0&3&-2\\
0&0&2&-1
\end{pmatrix}</script><p>求矩阵 $A$ 的 $Jordan$ 标准型。</p>
<h3 id="Jordan-标准型"><a href="#Jordan-标准型" class="headerlink" title="Jordan 标准型"></a>Jordan 标准型</h3><h4 id="初等因子组法"><a href="#初等因子组法" class="headerlink" title="初等因子组法"></a>初等因子组法</h4><p>这里介绍另外一个方法，相比课堂上的方法，在高维情况下更加适用，而在低维情况下，$Jordan$ 标准型其实可以通过简单的求代数重数和几何重数得到。</p>
<p>1） 将特征方阵 $\lambda I-A$ 化为 $Smith$ 标准形（第二、三初等变换仅限乘以常数和 $\lambda$ ），由此求得不变因子 $d_1(\lambda),…,d_n(\lambda)$ ；</p>
<script type="math/tex; mode=display">
\lambda I-A\sim\begin{pmatrix}
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&(\lambda-1)^2(\lambda+1)^2
\end{pmatrix}</script><p>其中 $d_1(\lambda)=d_2(\lambda)=d_3(\lambda)=1;d_4(\lambda)=(\lambda-1)^2(\lambda+1)^2$ 。</p>
<p>2） 把不变因子 $d_1(\lambda),…,d_n(\lambda)$ 分解为一次因子的乘积，求出初等因子组 $\lbrace (\lambda-\lambda_i)^{e_j} \rbrace$ ；</p>
<script type="math/tex; mode=display">
d_4(\lambda)=(\lambda-1)^2(\lambda+1)^2 \to (\lambda-1)^2,(\lambda+1)^2</script><p>3） 每个初等因子 $(\lambda-\lambda_i )^{e_j}$ 都对应着一个如下 $Jordan$ 块；</p>
<script type="math/tex; mode=display">
(\lambda-\lambda_i)^{e_j}\to J_{e_j}(\lambda_i)=
\begin{pmatrix}
\lambda_i&1&\cdots&0&0\\
0&\lambda_i&\cdots&0&0\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&\cdots&\lambda_i&1 \\
0&0&\cdots&0&\lambda_i\\
\end{pmatrix}_{e_j\times e_j}</script><script type="math/tex; mode=display">
(\lambda-1)^2\to\begin{pmatrix}
1&1\\
0&1\\
\end{pmatrix} \\
(\lambda+1)^2\to\begin{pmatrix}
-1&1\\
0&-1\\
\end{pmatrix}</script><p>4） 将  $Jordan$ 标准型即 $J=\text{diag}(J<em>{e_1}(\lambda_1),…,J</em>{e_k}(\lambda_s))$ 。</p>
<script type="math/tex; mode=display">
J=\begin{pmatrix}
1&1&0&0\\
0&1&0&0\\
0&0&-1&1\\
0&0&0&-1
\end{pmatrix}</script><h3 id="过渡矩阵-P"><a href="#过渡矩阵-P" class="headerlink" title="过渡矩阵 P"></a>过渡矩阵 P</h3><p>对于 $Jordan$ 块 $J_{e_j}(\lambda_i)$ 其对映的过渡矩阵部分为 $((A-\lambda_iI)^{e_j-1}\beta,…,(A-\lambda_iI)\beta,\beta)$ ，其中 $\beta$ 符合以下条件，</p>
<script type="math/tex; mode=display">
(A-\lambda_iI)^{e_j}\beta=0 \text{ and } (A-\lambda_iI)^{e_j-1}\beta\neq0</script>]]></content>
      <categories>
        <category>MatrixTheory</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
</search>
